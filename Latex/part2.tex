Give a short Outline

\section{Setup}
Introduction to this section!

\subsection{Choice of Datasets}
Luis' Taxonomy, verschiedene Bereiche -> TUDataset

\subsection{Choice of Models}
GIN weil sehr expressive \cite{Xu2018}, GAT und GCN wegen \cite{Nikolentzos2023} mit basic pool um complexity der models zu containen und bessere vergleiche zu haben

$\wlnn$ auch nur mit basic pool

\subsection{Experimental Setup}
python 3.10, pytorch und pytorch-geometric (use footnotes from outline). Use of Wheights and Biases as data logging instance, HPC, Colab und meine eigene Hardware - > summierte overall computation time. Standard procedure, training with 10-fold and repition and a split of training, validation and test dataset.

\section{Results}
Introduction to this section!

\subsection{Explain how results look like}
Shortly explain, their performance and how they typically learn. Grafiken über das mean train\_acc, val\_acc, test\_acc vs. epochs.

\subsubsection{1-WL+NN}
Explain most important parameters such as k-wl mit der wl accuracy on. Show how it relates to the datasets by showing the table of theoretical accuracies, 
Grafiken über das mean train\_acc, val\_acc, test\_acc vs. epochs.

\subsubsection{GNN}
Show performance of different GNN models -> Boxplot of different pooling functions
Grafiken über das mean train\_acc, val\_acc, test\_acc vs. epochs.

\subsection{Test Accuracy}
Show a big table with all test-accuracies and its standard deviaton and highlight in bold face, the best one.
Asses the std of both model types.

\subsection{Overfitting Behaviour}
Compare the difference between training data performance vs test accuracy accross both models

\subsection{Aggregate Analysis}
Explain what aggregates are and show tSNE with the of the best models with SVM

Also use KNN to assess how good the actuall aggregates are differentiable. -> local clusters ?

\section{Discusssion}
\subsection{Learned Lessons}
\subsection{Future Work}

\section{Conclusion}


\begin{table}
    \resizebox{.975\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccccc@{}}	\toprule
			& \multirow{3}{*}{\vspace*{4pt}\textbf{1-WL}}&\multicolumn{8}{c}{\textbf{Dataset}}\\\cmidrule{3-10}
			& & {\textsc{Enzymes}}         &  {\textsc{Imdb-Binary}}  & {\textsc{Imdb-Multi}}  & {\textsc{Mutag}}           & {\textsc{NCI1}}       & {\textsc{Proteins}}  & {\textsc{Reddit-Binary}} & {\textsc{Reddit-Multi (5K)}}
			\\
			\toprule
			\multirow{6}{*}{} 	&
			Iterations: $0$       &  81.4 & 60.6 & 44.2 & 93.1 & 91.3 & 91.9 & 83.9 & 55.1
			\\ 
			& Iterations: $1$    & 100.0 & 88.6 & 63.3 & 95.7 & 99.5 & 99.7 & 100.0 & 100
			\\
			& Iterations: $2$    & - & - & - & 99.5 & 99.8 & - & - & -
			\\
			& Iterations: $3$   & - & - & - & 100.0 & 99.8 & - & - & -
			\\
			& Iterations: $4$    & - & - & - & - & - & - & -
			\\
			\cmidrule{2-10}\morecmidrules\cmidrule{2-10}
			& Max Accuracy     & 100.0 & 88.6 & 63.3 & 100.0 & 99.8 & 99.7 & 100.0 & 100.0         
			\\
			\bottomrule
		\end{tabular}}
        \caption{An overview of the maximum theoretical accuracy achievable for each dataset based on the number of 1-WL iterations.}
        \label{tab:my_label2}                  
\end{table}

\begin{table}
    \resizebox{.975\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccc@{}}	\toprule
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{6}{c}{\textbf{Dataset}}\\\cmidrule{3-8}
			& & {\textsc{Enzymes}}         &  {\textsc{Imdb-Binary}}      & {\textsc{Mutag}}           & {\textsc{NCI1}}       & {\textsc{Proteins}}           & 
			{\textsc{Reddit-Binary}}
			\\
			\toprule
			\multirow{6}{*}{\rotatebox{90}{$\wlnn$}} 	&
			\textsf{Max} & 16.7 \scriptsize $\pm 4.2$ & 52.0 \scriptsize $\pm 5.3$	&73.8 \scriptsize $\pm 12.4$ &	58.6 \scriptsize $\pm 3.3$ & 62.9 \scriptsize $\pm 4.9$  & 	69.2 \scriptsize $\pm 4.0$
			\\ 
			& \textsf{Mean} & 18.2 \scriptsize $\pm 4.8$  &	59.4 \scriptsize $\pm 5.8$ &	77.1 \scriptsize $\pm 11.5$	 & 64.0 \scriptsize $\pm 3.3$ &	60.9 \scriptsize $\pm 4.5$ & 66.1 \scriptsize $\pm 3.2$   
			\\ 	
			& \textsf{Sum} & 18.0 \scriptsize $\pm 6.2$	 & 57.5 \scriptsize $\pm 5.1$ & 66.8 \scriptsize $\pm 13.9$ & 56.9 \scriptsize $\pm 3.8$ & 65.6 \scriptsize $\pm 4.8$ & 73.0 \scriptsize $\pm 5.1$   
			\\  
			\cmidrule{2-8}  		
			& \textsf{Embedding-Max} & 40.5  \scriptsize	$\pm 7.4$         & 69.4  \scriptsize $\pm 4.9$             & 0.0 \scriptsize $\pm 0.0$            & 82.7  \scriptsize $\pm 2.0$          & \textbf{75.2} \scriptsize $\pm 3.9$ & 0.0 \scriptsize $\pm 0.0$        
			\\ 
			& \textsf{Embedding-Mean}     & 42.6 \scriptsize	$\pm 9.0$ & \textbf{72.4}     \scriptsize $\pm 4.1 $         & 0.0 \scriptsize $\pm 0.0$            & 83.1   \scriptsize $\pm 1.9$         & 72.3  \scriptsize $\pm 4.2$         &  0.0 \scriptsize $\pm 0.0$                     
			\\ 
			& \textsf{Embedding-Sum} & \textbf{48.3} \scriptsize	$\pm 8.1$          & 72.0  \scriptsize $\pm 3.8$             & 0.0 \scriptsize $\pm 0.0$            & \textbf{83.6}   \scriptsize $\pm 2.2$         & 75.2  \scriptsize $\pm 4.5$   	& 0.0  \scriptsize $\pm 0.0$                     
			\\ 
			\cmidrule{2-8}
			\multirow{9}{*}{\rotatebox{90}{Graph Neural Networks}} 
			& \textsf{GAT:Max}                    & 31.2 \scriptsize $\pm 6.0$        & 70.7 \scriptsize $\pm 4.8$          & 71.1 \scriptsize $\pm 12.2$ & 58.0 \scriptsize $\pm 4.2$          & 72.5 \scriptsize $\pm 5.1$         & 0.0 \scriptsize $\pm 0.0$  
			\\ 
			& \textsf{GAT:Mean}    & 28.9 \scriptsize $\pm 5.9$          & 70.9 \scriptsize $\pm 3.7$           & 74.8 \scriptsize $\pm 9.1$            & 66.1 \scriptsize $\pm 2.8$         & 64.9 \scriptsize $\pm 6.4$       & 0.0 \scriptsize $\pm 0.0$
			\\ 
			& \textsf{GAT:Sum}                  & \textbf{34.4} \scriptsize $\pm 7.0$          & 72.2 \scriptsize $\pm 4.5$	            & 82.1 \scriptsize $\pm 11.2$            & 69.8 \scriptsize $\pm 2.6$	          & 73.4 \scriptsize $\pm 3.9$
			& 0.0 \scriptsize $\pm 0.0$          
			\\
			
			\cmidrule{2-8}
					
			& \textsf{GCN:Max} & 33.1 \scriptsize $\pm 7.5$ &	73.5 \scriptsize $\pm 4.1$	& 74.5 \scriptsize $\pm 11.3$ & 61.1 \scriptsize $\pm 3.6$ &	69.8 \scriptsize $\pm 5.9$ & 0.0 \scriptsize $\pm 0.0$  
			\\ 
			& \textsf{GCN:Mean} & 29.9 \scriptsize $\pm 5.7$ &	\textbf{74.7} \scriptsize $\pm 3.8$ & 75.0 \scriptsize $\pm 10.4$ &	68.9 \scriptsize $\pm 2.4$ &	70.9 \scriptsize $\pm 5.2$ & 0.0 \scriptsize $\pm 0.0$
			\\ 
			& \textsf{GCN:Sum} & 31.7 \scriptsize $\pm 7.2$ &	73.0 \scriptsize $\pm 4.4$	& 81.5 \scriptsize $\pm 10.3$ & 70.4 \scriptsize $\pm 2.1$ & 3.5 \scriptsize $\pm 3.9$ & 0.0 \scriptsize $\pm 0.0$                        
			\\
			\cmidrule{2-8}	
						
			& \textsf{GIN:Max} & 29.2 \scriptsize $\pm 6.2$	& 70.8 \scriptsize $\pm 4.7$ & 77.3 \scriptsize $\pm 10.7$ & \textbf{79.9} \scriptsize $\pm 2.2$ & \textbf{74.3} \scriptsize $\pm 5.1$ & 0.0 \scriptsize $\pm 0.0$   
			\\ 
			& \textsf{GIN:Mean}  & 31.7 \scriptsize $\pm 6.7$	& 71.1 \scriptsize $\pm 5.4$ & 82.4 \scriptsize $\pm 9.8$ & 	70.8 \scriptsize $\pm 2.2$ & 72.0 \scriptsize $\pm 4.0$ & 0.0 \scriptsize $\pm 0.0$
			\\ 
			& \textsf{GIN:Sum} & 28.9 \scriptsize $\pm 8.7$ & 	69.5 \scriptsize $\pm 4.8$	& \textbf{84.6} \scriptsize $\pm 8.7$ & 70.8 \scriptsize $\pm 2.3$ &	73.2 \scriptsize $\pm 4.3$ & 0.0 \scriptsize $\pm 0.0$
			\\
			\bottomrule
		\end{tabular}}
        \caption{Overview of the classification accuracies achieved by the best model of each configuration for each dataset in percent and standard deviation.}
        \label{tab:my_label}                  
\end{table}

\begin{table}
	\resizebox{0.49\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
	\newcommand{\seq}[2]{$(#1, #2)$\text{-}\textsf{SpeqNet}\xspace}
		\begin{tabular}{@{}lcc@{}}	\toprule
			\multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{2}{c}{\textbf{Dataset}}
			\\
			\cmidrule{2-3} 
			& {\textsc{Alchemy (10k)}} & {\textsc{Zinc?}} 
			\\	
			\toprule
			\textsf{GINE-$\varepsilon^*$}\xspace        & 0.180 {\scriptsize $\pm  0.006$} -1.958  {\scriptsize $\pm  0.047$}
			\\		
			\text{\seq{2}{1}}$^*$   &  0.169 {\scriptsize $\pm 0.005$} -2.010    {\scriptsize $\pm 0.056$}
			\\
			\text{\seq{2}{2}}$^*$    & \textbf{0.115} {\scriptsize $\pm 0.001$} -2.722  {\scriptsize $\pm 0.054$}

			\\
			\cmidrule{1-3}
			\textsf{Embedding-Max} & 0.409 {\scriptsize $\pm  0.003$} -1.023 {\scriptsize $\pm  0.009$}
			\\
			\textsf{Embedding-Mean} & 0.355 {\scriptsize $\pm  0.004$} -1.269  {\scriptsize $\pm  0.020$}
			\\
			\textsf{Embedding-Sum} & \textbf{0.305} {\scriptsize $\pm  0.001$} -1.740 {\scriptsize $\pm  0.042$}
			\\
			\bottomrule
	\end{tabular}}
	\caption{Mean MAE (mean std.\ MAE, logMAE) on large-scale (multi-target) molecular regression tasks. The results of the models marked $^*$ are taken from \cite{Morris2022}}
\end{table}

\begin{table}
    \resizebox{.975\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.05}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccc@{}}	\toprule
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{6}{c}{\textbf{Dataset}}\\\cmidrule{3-8}
			& & {\textsc{Enzymes}}         &  {\textsc{Imdb-Binary}}      & {\textsc{Mutag}}           & {\textsc{NCI1}}       & {\textsc{Proteins}}           & 
			{\textsc{Reddit-Binary}}
			\\
			\toprule
			\multirow{4}{*}{\rotatebox{90}{$\wlnn$}}
			& \textsf{MLP} & 48.3 \scriptsize	$\pm 8.1$ & \textbf{72.4} \scriptsize	$\pm 4.1$ &  & 83.6 \scriptsize	$\pm 4.1$ & \textbf{75.2} \scriptsize $\pm 3.9$
			\\
			& \textsf{SVM Linear} & 34.4 \scriptsize	$\pm 5.5$ & 71.2 \scriptsize	$\pm 3.9$ &  & 83.4 \scriptsize	$\pm 2.1$ & 73.9 \scriptsize $\pm 4.1$
			\\
			& \textsf{SVM RBF} & 45.0 \scriptsize	$\pm 7.0$ & 72.8 \scriptsize	$\pm 4.3$ &  & 83.6 \scriptsize	$\pm 1.9$ & 75.2 \scriptsize $\pm 4.0$
			\\
			& \textsf{k-NN} &  \textbf{56.3} \scriptsize $\pm 5.8$ (k=1) & 72.3 \scriptsize $\pm 4.1$ (k=11)&	& \textbf{83.9} \scriptsize $\pm 1.8$ (k=5)& 73.9 \scriptsize $\pm 4.1$ (k=19) &
			\\
			\cmidrule{2-8}
			\multirow{4}{*}{\rotatebox{90}{GNN}}
			& \textsf{MLP} & 34.4 \scriptsize $\pm 7.0$ &  \textbf{74.7} \scriptsize $\pm 3.8$ & & \textbf{79.9} \scriptsize $\pm 2.2$ & 74.3 \scriptsize $\pm 5.1$
			\\
			& \textsf{SVM Linear} & 33.2 \scriptsize $\pm 5.9$ & 73.9 \scriptsize $\pm 4.2$ & & 67.4 \scriptsize $\pm 2.2$ & 74.7 \scriptsize $\pm 4.2$
			\\
			& \textsf{SVM RBF} & 35.9 \scriptsize $\pm 6.0$ & 74.1 \scriptsize $\pm 3.9$ & & 73.0 \scriptsize $\pm 1.9$ & 74.6 \scriptsize $\pm 4.6$
			\\ 
			& \textsf{k-NN} & \textbf{51.6} \scriptsize $\pm 7.0$ (k=1)	& 74.3 \scriptsize $\pm 4.0$ (k=132) & & 77.5 \scriptsize $\pm 1.7$	(k=2)& \textbf{74.9} \scriptsize $\pm 4.3$ (k=27)&
			\\
			\bottomrule
		\end{tabular}}
		\caption{Overview of the classification accuracies achieved by the best model for each dataset in percent and standard deviation. Additionally, the performance of each model was evaluated under alternative configurations, namely the substitution of the Multilayer Perceptron (MLP) with a Support Vector Machine employing either a linear kernel (SVM Linear) or the Radial Basis Function (SVM RBF), as well as the k-nearest neighbors algorithm (k-NN) with different values for $k$.}
        \label{tab:my_label3}                  
\end{table}

\begin{figure}
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/tsne_svm_lin_ENZYMES.pdf}
		\vspace*{-4ex} 
		\caption{\textsc{Enzymes}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/tsne_svm_lin_IMDB.pdf}
		\vspace*{-4ex} 
		\caption{\textsc{Imdb-Binary}}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/tsne_svm_lin_IMDB.pdf}
		\vspace*{-4ex} 
		\caption{\textsc{Mutag}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/tsne_svm_lin_NCI1.pdf}
		\vspace*{-4ex} 
		\caption{\textsc{Nci1}}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/tsne_svm_lin_PROTEINS.pdf}
		\vspace*{-4ex} 
		\caption{\textsc{Proteins}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/tsne_svm_lin_ENZYMES.pdf}
		\vspace*{-4ex} 
		\caption{\textsc{Reddit-Binary}}
	\end{subfigure}
	\caption{Visualization of the SVM Linear and tsne plots!!}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Figures/knn.pdf}
	\caption{k-nearest neighbors algorithm!}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/heatmaps_ENZYMES_single.pdf}
		\vspace*{-5ex} 
        \caption{\textsc{Enzymes}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/heatmaps_IMDB-BINARY_single.pdf}
		\vspace*{-5ex} 
        \caption{\textsc{Imdb-Binary}}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/heatmaps_IMDB-BINARY_single.pdf}
		\vspace*{-5ex} 
        \caption{\textsc{Mutag}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/heatmaps_NCI1_single.pdf}
		\vspace*{-5ex} 
        \caption{\textsc{Nci1}}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/heatmaps_PROTEINS_single.pdf}
		\vspace*{-5ex} 
        \caption{\textsc{Proteins}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/heatmaps_IMDB-BINARY_single.pdf}
		\vspace*{-5ex}
        \caption{\textsc{Reddit-Binary}}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[b]{0.8\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/colorbar.pdf}
	\end{subfigure}
	\caption{Visualizing the performance of the best-performing GNN models of each dataset in approximating node colors computed by the 1-WL algorithm.}
\end{figure}

