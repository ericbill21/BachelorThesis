\section{Background and Related Work}\label{sec:related_work}
In this section, we will briefly introduce the foundation of our research by explaining the origins of each method, mentioning important recent advances, and providing a brief overview of the connections between them.

\subsection{Weisfeiler-Leman Algorithm}
The (1-dimensional) Weisfeiler-Leman algorithm (1-WL), proposed by \cite{Wei+1968}, was initially designed as a simple heuristic for the \textit{graph isomorphism problem}, but due to its interesting properties, its simplicity, and its good performance, the 1-WL algorithm gained a lot of attention from researchers across many fields. One of the most noticeable properties is that the algorithm color codes the nodes of the input graph in such a way that in each iteration, each color encodes a learned local substructure.

It works by coloring all nodes in each iteration the same color that fulfills two properties: 1. the nodes already share the same color, and 2. the count of each color of their direct neighbors is equal. The algorithm continues as long as the number of colors changes in each iteration.
For determining whether two graphs are non-isomorphic, the heuristic is applied to both graphs simultaneously. It concludes that the graphs are non-isomorphic as soon as the number of occurrences of a color differs between them. We present a more formal definition of the algorithm in the following part in \autoref{sec:1-WL Definition}.

Since the \textit{graph isomorphism problem} is difficult to solve due to the best known complete algorithm only running in deterministic quasipolynomial time (\cite{Babai2016}), the 1-WL algorithm, running in deterministic polynomial time, cannot solve the problem completely. Moreover, \cite{Cai1992} constructed counterexamples of non-isomorphic graphs that the heuristic fails to distinguish, e.g., see \autoref{1-WL Counter Example}. However, following the work of \cite{Bab+1979}, this simple heuristic is still quite powerful and has a very low probability of failing to distinguish non-isomorphic graphs when both graphs are uniformly chosen at random as the number of nodes tends to infinity.

To overcome the limited expressiveness of the 1-WL algorithm, it has been generalized to the $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) by \cite{Bab1979, Babai2016}, as well as \cite{Imm+1990}\footnote{In \cite{Babai2016} on page 27, László Babai explains that he, together with Rudolf Mathon, first introduced this algorithm in 1979. He adds that the work of \cite{Imm+1990} introduced this algorithm independently of him.}. This version works with $k$-tuples over the $k$-ary Cartesian product of the set of nodes. Interestingly, this created a hierarchy for the expressiveness of determining non-isomorphism, such that for all $k \in \mathbb{N}$ there exists a pair of non-isomorphic graphs that can be distinguished by the $(k+1)$-WL but not by the $k$-WL (\cite{Cai1992}).

\subsection{Graph Neural Networks}
The utilization of machine learning techniques, previously proven effective in various domains, for graph analysis has been a well-established topic in the literature for several decades. However, researchers faced challenges in effectively adapting these techniques to graphs of diverse sizes and complexities in the early stages. Notably, the seminal works by Sperduti (1997), Scarselli (2008), and Micheli (2009) emerged as prominent examples of successful applications in this regard.


The idea of leveraging machine learning techniques, previously proven effective in various domains, for graph-related tasks has been a well-established topic in the literature for the past decades. However, in the early stages, researchers faced challenges in effectively adapting these techniques to work on graphs of arbitrary sizes and complexities. Notably, the works by \cite{Sperduti1997,Scarselli2008}, and \cite{Micheli2009} were the first prominent examples of successful applications in this regard.

However, it was not until the emergence of more advanced models that the scientific community truly recognized the significance and potential of Graph Neural Networks (GNNs). Noteworthy among these advancements were the work of \cite{Duvenaud2015}, who introduced a differentiable approach for generating unique fingerprints of arbitrary graphs, as well as \cite{Li2015}, who applied gated recurrent units to capture graphs of various sizes, while \cite{Atwood2016} utilized diffusional convolutions for the same purpose. Of particular significance, however, were the contributions of \cite{Bruna2013,Defferrard2016} and \cite{Kip+2017}, which extended the concept of convolution from its traditional application on images to the domain of arbitrary graphs.

After the early success of these GNNs, \cite{Gil+2017} introduced a unified architecture for GNNs. The authors observed a recurring pattern in how information is exchanged and processed among many of these works, including many mentioned in the paragraph above. Leveraging these observations, \cite{Gil+2017} devised the message-passing architecture as a generalized framework for GNNs. Models using this architecture can be referred to as Message-Passing-Neural-Network (MPNN); however, throughout this thesis, we will use the term GNN and MPNN interchangeably, as the focus of this thesis is solely on the message-passing architecture. 
This architecture uses the input graph as its basis for computation and computes new node features for the graph in each layer. Each new node feature is derived by aggregating the nodes and neighboring node features. After applying each layer of a GNN model, a representation of the entire graph is obtained by applying a pooling function (see YingMorris2018). This representation is then further processed by common machine learning techniques like a multilayer perceptron for the final output. We will present a more formal definition of this architecture in the following part; however, important to note is that the information exchange in the graph across nodes is limited to a one-hop neighbor per layer.
 
With this general framework and the empirical success of some models using this message-passing architecture, the question of how expressive models based on this architecture can be gained a lot of attention in the scientific community. Many papers immediately established connections to the 1-WL algorithm, among the most prominent being \cite{Morris2018} and \cite{Xu2018}. These connections seem natural, as both methods share similar properties in terms of how they process graph data. Most strikingly, both methods never change the graph structurally since they only compute new node features in each iteration. Moreover, both methods use a one-hop neighborhood aggregation as the basis for computing the new node feature. Following this intuition, \cite{Morris2018} as well as \cite{Xu2018} showed that the expressiveness of GNN is upper-bounded by the 1-WL in terms of distinguishing non-isomorphic graphs. Moreover, \cite{Morris2018} proposed a new $k$-GNN architecture that operates over the set of subgraphs of size $k$. Interestingly, \cite{Geerts2020} as well as \cite{Gro2017} have shown that the proposed hierarchy over $k \in \mathbb{N}$ is equivalent to the $k$-WL hierarchy in terms of its ability to distinguish non-isomorphic graphs, i.e., if there is a $k$-GNN that can distinguish two non-isomorphic graphs, it is equivalent to say that the $k$-WL algorithm can also distinguish these graphs.

Although there are other modifications of the message-passing architecture besides the theoretical concept of $k$-GNN to increase the expressiveness of GNNs in terms of distinguishing non-isomorphism, e.g., using node identifiers \cite{Vignac2020}, adding random node features \cite{Sato2021,Abboud2020}, adding directed flows \cite{Beaini2021} and many more. Relatively few works have been published that attempt to understand the representation learned from a standard GNN.

Notable works include \cite{Nikolentzos2023weisfeiler}, where the author, in addition to the normal learning process, optimized GNNs to preserve a notion of distance in their representation and examined the effectiveness of GNNs in utilizing such representations. However, their insights can only be applied to these specially trained GNNs, not standard ones. In another publication, \cite{Nikolentzos2023} presented mathematical proof and empirical confirmation showing how much structural information is encoded by modern GNN models. The research highlights that GNN models like DGCNN (\cite{Zhang2018}) and GAT (\cite{Velivckovic2017}) encode all nodes with the same feature vector, while in contrast, models like GCN (\cite{Kip+2017}) and GIN (\cite{Xu2018}) encode nodes after $k$ layer of message-passing with features that relate with the number of walks of length $k$ form each node, disregarding the local structure within the nodes are contained.