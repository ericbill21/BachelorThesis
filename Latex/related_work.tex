\section{Background and Related Work}\label{sec:related_work}
In this section, we will briefly introduce the foundation of our research by explaining the origins of the two frameworks, mentioning important recent advances, and providing a brief overview of the connections between them.

\subsection{Weisfeiler-Leman Algorithm}\label{sec:related_work_wl}
The (1-dimensional) Weisfeiler-Leman algorithm (\wl), proposed by \cite{Wei+1968}, was initially designed as a simple heuristic for the \textit{graph isomorphism problem}, but due to its interesting properties, its simplicity, and its good performance, the \wl algorithm gained much attention from researchers across many fields. One of the most noticeable properties is that the algorithm color codes the nodes of the input graph in such a way that in each iteration, each color encodes a learned local substructure.

This algorithm functions by assigning the same color to all nodes that meet two criteria: 1) they already share the same color, and 2) each color appears equally often in the set of the node's direct neighbors. The algorithm continues until the number of colors changes in each iteration. For determining whether two graphs are non-isomorphic, the heuristic is applied to both graphs simultaneously. The heuristic concludes that the graphs are non-isomorphic as soon as the number of occurrences of a color differs between them. We present a more formal definition of the algorithm in the following part in \cref{sec:1-WL Definition}.

Since the \textit{graph isomorphism problem} is difficult to solve due to the best known complete algorithm only running in deterministic quasipolynomial time (\cite{Babai2016}), the \wl algorithm, running in deterministic polynomial time, cannot solve the problem completely. Moreover, \cite{Cai1992} constructed counterexamples of non-isomorphic graphs that the heuristic fails to distinguish, e.g., see \cref{1-WL Counter Example}. However, following the work of \cite{Bab+1979}, this simple heuristic is still quite powerful and has a very low probability of failing to distinguish non-isomorphic graphs when both graphs are uniformly chosen at random as the number of nodes tends to infinity.

To overcome the limited expressiveness of the \wl algorithm, it has been generalized to the $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) by \cite{Bab1979, Babai2016}, as well as \cite{Imm+1990}\footnote{In \cite{Babai2016} on page 27, László Babai explains that he, together with Rudolf Mathon, first introduced this algorithm in 1979. He adds that the work of \cite{Imm+1990} introduced this algorithm independently of him.}. This version works with $k$-tuples over the $k$-ary Cartesian product of the set of nodes. Interestingly, this created a hierarchy for the expressiveness of determining non-isomorphism, such that for all $k \in \mathbb{N}$ there exists a pair of non-isomorphic graphs that can be distinguished by the \textsf{$(k+1)$-WL} but not by the \textsf{$k$-WL} (\cite{Cai1992}).

\subsection{Graph Neural Networks}
The idea of leveraging machine learning techniques, previously proven effective in various domains, for graph-related tasks has been a well-established topic in the literature for the past decades. However, researchers faced challenges in effectively adapting these techniques to graphs of diverse sizes and complexities in the early stages. Notably, the works by \cite{Sperduti1997,Scarselli2008}, and \cite{Micheli2009} were the first prominent examples of successful applications in this regard.

However, it was not until the emergence of more advanced models that the scientific community truly recognized the significance and potential of \textsf{Graph Neural Networks} (\gnns). Noteworthy among these advancements were the work of \cite{Duvenaud2015}, who introduced a differentiable approach for generating unique fingerprints of arbitrary graphs, as well as \cite{Li2015}, who applied gated recurrent units to capture graphs of various sizes, while \cite{Atwood2016} utilized diffusional convolutions for the same purpose. Of particular significance, however, were the contributions of \cite{Bruna2013,Defferrard2016} and \cite{Kip+2017}, which extended the concept of convolution from its traditional application on images to the domain of arbitrary graphs.

After the early success of these \gnn models, \cite{Gil+2017} introduced a unified architecture for \gnns. The authors observed a recurring pattern in how information is exchanged and processed among many of these works, including many mentioned in the paragraph above. Leveraging these observations, \cite{Gil+2017} devised the message-passing architecture as a generalized framework for \gnns. Models using this architecture can be referred to as Message-Passing-Neural-Network (\textsf{MPNN}); however, throughout this thesis, we will use the term \gnn and \textsf{MPNN} interchangeably, as the focus of this thesis is solely on the message-passing architecture. 
This architecture uses the input graph as its basis for computation and computes new node features for the graph in each layer. The computation of each new node feature involves aggregating all the features of the neighboring nodes and the node's own feature. After applying each layer of a \gnn model, a representation of the entire graph is obtained by applying a pooling function (e.g. \cite{Ying2018}). This representation is then further processed by common machine learning techniques like a multilayer perceptron for the final output. We will present a more formal definition of this architecture in the following part in \cref{sec:GNN Defintion}; however, important to note is that the information exchange in the graph across nodes is limited to a one-hop neighbor per layer.
 
With this general framework and the empirical success of some models using this message-passing architecture, the question of how expressive models based on this architecture can be gained a lot of attention in the scientific community. Many papers immediately established connections to the \wl algorithm, among the most prominent being \cite{Morris2018} and \cite{Xu2018}. These connections seem natural, as both methods share similar properties in terms of how they process graph data. Most strikingly, both methods never change the graph structurally since they only compute new node features in each iteration. Moreover, both methods use a one-hop neighborhood aggregation as the basis for computing the new node feature. Following this intuition, \cite{Morris2018}, as well as \cite{Xu2018}, showed that the expressiveness of \gnns is upper-bounded by the \wl in terms of distinguishing non-isomorphic graphs. Moreover, \cite{Morris2018} proposed a new $k$-\gnn architecture that operates over the set of subgraphs of size $k$. Interestingly, \cite{Geerts2020} has shown that this architecture imposes a hierarchy over $k \in \mathbb{N}$ that is equivalent to the $k$-WL hierarchy in terms of its ability to distinguish non-isomorphic graphs, i.e., if there is a $k$-\gnn that can distinguish two non-isomorphic graphs, it is equivalent to say that the $k$-WL algorithm can also distinguish these graphs.

Although there are other modifications of the message-passing architecture besides the theoretical concept of $k$-\gnn to increase the expressiveness of \gnns in terms of distinguishing non-isomorphism, e.g., using node identifiers \cite{Vignac2020}, adding random node features \cite{Sato2021,Abboud2020}, adding directed flows \cite{Beaini2021} and many more. Relatively few works have been published that attempt to understand the representation learned from a standard \gnn.

Notable works include \cite{Nikolentzos2023weisfeiler}, where the authors, in addition to the normal learning process, optimized \gnns to preserve a notion of distance in their representation and examined the effectiveness of \gnns in utilizing such representations. However, their insights can only be applied to these specially trained \gnn models and not be generalized. In another publication, \cite{Nikolentzos2023} presented mathematical proof and empirical confirmation showing how much structural information is encoded by modern \gnn models. Their research highlights that \gnn models like DGCNN (\cite{Zhang2018}) and GAT (\cite{Velivckovic2017}) encode all nodes with the same feature vector, while in contrast, models like GCN (\cite{Kip+2017}) and GIN (\cite{Xu2018}) encode nodes after $k$ layers of message-passing with features that relate with the number of walks of length $k$ over the input graph form each node, disregarding the local structure within the nodes are contained.