\section{Background and Related Work}\label{sec:related_work}
In this section, we will briefly introduce the foundation of our research by explaining the origins of each method, mentioning recent important advances, and providing a brief overview of connections between them.


 

\subsection{Weisfeiler-Leman Algorithm}
The (1-dimensional) Weisfeiler-Leman algorithm (1-WL), proposed by \cite{Wei+1968}, was initially designed as a simple heuristic for the \textit{graph isomorphism problem}, but due to its interesting properties, its simplicity, and its good performance, the 1-WL algorithm gained a lot of attention from researchers across many fields. One of the most noticeable of these properties is, that the algorithm color codes the nodes of the input graph in such a way, that in each iteration, each color encodes a learned local substructure.

It works by coloring all nodes in each iteration the same color that fulfills two properties: 1. the nodes already share the same color and 2. the count of each color of their direct neighbors is equal. The algorithm continues as long as the number of colors changes in each iteration.
For determining whether two graphs are non-isomorphic, the heuristic is applied to both graphs simultaneously and concludes that the graphs are non-isomorphic as soon as the number of occurrences of a color is different between the graphs. We present a more formal definition of the algorithm later in the \autoref{sec:1-WL Definition}.

Since the \textit{graph isomorphism problem} is difficult to solve due to the best known complete algorithm only running in deterministic quasipolynomial time (\cite{Babai2016}), the 1-WL algorithm, running in polynomial deterministic time, cannot solve the problem completely. Moreover, \cite{Cai1992} constructed counterexamples of non-isomorphic graphs that the heuristic fails to distinguish, e.g. see \autoref{1-WL Counter Example}. However, following the work of \cite{Bab+1979}, this simple heuristic is still quite powerful and has a very low probability of failing to distinguish non-isomorphic graphs when both graphs are uniformly chosen at random as the number of nodes tends to infinity.

To overcome the limited expressiveness of the 1-WL algorithm, it has been generalized to the $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) by \cite{Bab1979, Babai2016}, as well as \cite{Imm+1990}\footnote{In \cite{Babai2016} on page 27, László Babai explains that he, together with Rudolf Mathon, first introduced this algorithm in 1979. He adds, that the work of \cite{Imm+1990} introduced this algorithm independently of him.}. This version works with $k$-tuples over the $k$-ary Cartesian product of the set of nodes. Interestingly, this created a hierarchy for the expressiveness of determining non-isomorphism, such that for all $k \in \mathbb{N}$ there exists a pair of non-isomorphic graphs that can be distinguished by the $(k+1)$-WL but not by the $k$-WL (\cite{Cai1992}).

\subsection{Graph Neural Networks}
The idea of applying machine learning techniques that have been successful in other areas has been extensively explored in the literature over the past decades. However, one problem connected to this idea is how to handle graphs of arbitrary size and complexity as input for these proven techniques. One of the earliest successful works in doing so was \cite{Sperduti1997,Scarselli2008}, and \cite{Micheli2009}.

However, it was not until the emergence of more advanced models that the scientific community truly recognized the significance and potential of Graph Neural Networks (GNNs). Noteworthy among these advancements is the work of \cite{Duvenaud2015}, who introduced a differentiable approach for generating unique fingerprints of arbitrary graphs, as well as \cite{Li2015}, who applied gated recurrent units to capture graphs of various sizes, while \cite{Atwood2016} utilized diffusional convolutions for doing so. Particularly essential are the contributions made by \cite{Bruna2013,Defferrard2016}, and \cite{Kip+2017}, which extended the convolutional concept from its conventional application on images to the realm of arbitrary graphs.

After the early success of these GNNs, \cite{Gil+2017} introduced a unified architecture for GNNs. The authors observed a recurring pattern in how information is exchanged and processed among many of these works, including those mentioned in the paragraph above (except for the work of  \cite{Atwood2016}). Leveraging these observations, \cite{Gil+2017} devised the message-passing architecture as a generalized framework for GNNs. This architecture works by using the input graph as its basis for computation and computes in each layer new node features. Each node feature is derived by aggregating the nodes and neighboring node features. After every layer of a GNN model has been applied, a representation of the entire graph is obtained by applying a pooling function (cite YingMorris2018), which is then further processed by common machine learning practices like a multi-layer perceptron for the final output. We will present a more formal definition of this architecture in the following part; however, important to note is that the information exchange in the graph across nodes is limited to a one-hop neighbor per layer.

With this general framework and the empirical success of some models utilizing this message-passing architecture, the question of how expressive models based on this architecture can be, gained a lot of attention in the scientific community. Many works immediately established connections to the 1-WL algorithm, among the most inlfuencable were \cite{Morris2018} and \cite{Xu2018}. These connections seems natural since both methods share similar properties in terms of how they process graph data. Most noticeably, both methods never change the graph structurally since they only compute new node features in each iteration. Moreover, both methods use a one-hop neighborhood aggregation as their basis for the computation of the new node feature. Following this intuition, \cite{Morris2018}, as well as \cite{Xu2018}, showed that GNN's expressiveness power is upper bounded by the 1-WL in terms of distinguishing non-isomorphic graphs. In addition, \cite{Morris2018} also proposed a new $k$-GNN architecture that works over the set of subgraphs of size $k$. Interestingly, \cite{Geerts2020}, as well as \cite{Gro2017}, showed that the proposed hierarchy over $k \in \mathbb{N}$ is equivalent to the $k$-WL hierarchy in terms of their capacity in distinguishing non-isomorphic graphs, meaning if there exists a $k$-GNN that can distinguish two non-isomorphic graphs than it is equivalent to say that the $k$-WL algorithm can distinguish these graphs as well.

Although there has been beside the concept of $k$-GNN, other variants to increase the expressiveness of GNNs in terms of distinguishing non-isomorphism, for example, using node identifiers \cite{Vignac2020}, adding random node features \cite{Sato2021,Abboud2020}, adding directional flow \cite{Beaini2021} and many more. There have been relatively few works published that aim to try to understand the representation learned by a GNN. Noteworthy works, include \cite{Nikolentzos2023} as well as \cite{Nikolentzos2023weisfeiler}


A lot of other works has used techniques to make GNNs more powerful than 1-WL in terms of distinguish non-isomorphism.  Blablab. some works tend to ivestigate representation learned by GNNs

Add somewhere that we call MPNNs from now on GNNs