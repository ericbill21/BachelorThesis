{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${fileDirname}"
        },
        {
            "name": "Python: WLNN",
            "type": "python",
            "request": "launch",
            "cwd": "${workspaceFolder}/Code",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args": ["--dataset",
                    "PROTEINS",
                    "--max_epochs",
                    "200",
                    "--batch_size",
                    "33",
                    "--lr",
                    "0.02",
                    "--k_fold",
                    "5",
                    "--k_wl",
                    "3",
                    "--model",
                    "1WL+NN:Sum",
                    "--wl_convergence",
                    "True",
                    "--encoding_kwargs"
                    "{'embedding_dim' : 64}",
                    "--mlp_kwargs",
                    "{'num_layers' : 6, 'dropout' : 0.1, 'act' : 'relu', 'norm' : 'batch_norm', 'hidden_channels' : 64}"]
        },
        {
            "name": "Python: WLNN Regression",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args": ["--max_epochs",
                    "200",
                    "--batch_size",
                    "32",
                    "--lr",
                    "0.02",
                    "--k_fold",
                    "5",
                    "--k_wl",
                    "3",
                    "--model",
                    "1WL+NN:Embedding-Sum",
                    "--wl_convergence",
                    "True",
                    "--encoding_kwargs"
                    "{'embedding_dim' : 64}",
                    "--mlp_kwargs",
                    "{'num_layers' : 6, 'dropout' : 0.1, 'act' : 'relu', 'norm' : 'batch_norm', 'hidden_channels' : 64}"]
        },
    ]
}